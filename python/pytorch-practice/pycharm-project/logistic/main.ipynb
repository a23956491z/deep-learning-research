{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download MNIST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "dataset = MNIST(root='data/', download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get test dataset only"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "\n",
    "test_dataset = MNIST(root='data/', train=False)\n",
    "print(len(test_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "(<PIL.Image.Image image mode=L size=28x28 at 0x7F16FEEDC710>, 7)"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MNIST we download are PIL.Image object\n",
    "Though we can Display it\n",
    "however we can't just put into learning model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN80lEQVR4nO3df6hcdXrH8c+ncf3DrBpTMYasNhuRWBWbLRqLSl2RrD9QNOqWDVgsBrN/GHChhEr6xyolEuqP0qAsuYu6sWyzLqgYZVkVo6ZFCF5j1JjU1YrdjV6SSozG+KtJnv5xT+Su3vnOzcyZOZP7vF9wmZnzzJnzcLife87Md879OiIEYPL7k6YbANAfhB1IgrADSRB2IAnCDiRxRD83ZpuP/oEeiwiPt7yrI7vtS22/aftt27d281oAesudjrPbniLpd5IWSNou6SVJiyJia2EdjuxAj/XiyD5f0tsR8U5EfCnpV5Ku6uL1APRQN2GfJekPYx5vr5b9EdtLbA/bHu5iWwC61M0HdOOdKnzjND0ihiQNSZzGA03q5si+XdJJYx5/R9L73bUDoFe6CftLkk61/V3bR0r6kaR19bQFoG4dn8ZHxD7bSyU9JWmKpAci4o3aOgNQq46H3jraGO/ZgZ7ryZdqABw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUii4ymbcXiYMmVKsX7sscf2dPtLly5tWTvqqKOK686dO7dYv/nmm4v1u+66q2Vt0aJFxXU///zzYn3lypXF+u23316sN6GrsNt+V9IeSfsl7YuIs+toCkD96jiyXxQRH9TwOgB6iPfsQBLdhj0kPW37ZdtLxnuC7SW2h20Pd7ktAF3o9jT+/Ih43/YJkp6x/V8RsWHsEyJiSNKQJNmOLrcHoENdHdkj4v3qdqekxyTNr6MpAPXrOOy2p9o++uB9ST+QtKWuxgDUq5vT+BmSHrN98HX+PSJ+W0tXk8zJJ59crB955JHF+nnnnVesX3DBBS1r06ZNK6577bXXFutN2r59e7G+atWqYn3hwoUta3v27Cmu++qrrxbrL7zwQrE+iDoOe0S8I+kvauwFQA8x9AYkQdiBJAg7kARhB5Ig7EASjujfl9om6zfo5s2bV6yvX7++WO/1ZaaD6sCBA8X6jTfeWKx/8sknHW97ZGSkWP/www+L9TfffLPjbfdaRHi85RzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtlrMH369GJ948aNxfqcOXPqbKdW7XrfvXt3sX7RRRe1rH355ZfFdbN+/6BbjLMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJM2VyDXbt2FevLli0r1q+44opi/ZVXXinW2/1L5ZLNmzcX6wsWLCjW9+7dW6yfccYZLWu33HJLcV3UiyM7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTB9ewD4JhjjinW200vvHr16pa1xYsXF9e9/vrri/W1a9cW6xg8HV/PbvsB2zttbxmzbLrtZ2y/Vd0eV2ezAOo3kdP4X0i69GvLbpX0bEScKunZ6jGAAdY27BGxQdLXvw96laQ11f01kq6uty0Adev0u/EzImJEkiJixPYJrZ5oe4mkJR1uB0BNen4hTEQMSRqS+IAOaFKnQ287bM+UpOp2Z30tAeiFTsO+TtIN1f0bJD1eTzsAeqXtabzttZK+L+l429sl/VTSSkm/tr1Y0u8l/bCXTU52H3/8cVfrf/TRRx2ve9NNNxXrDz/8cLHebo51DI62YY+IRS1KF9fcC4Ae4uuyQBKEHUiCsANJEHYgCcIOJMElrpPA1KlTW9aeeOKJ4roXXnhhsX7ZZZcV608//XSxjv5jymYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9knulFNOKdY3bdpUrO/evbtYf+6554r14eHhlrX77ruvuG4/fzcnE8bZgeQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJxtmTW7hwYbH+4IMPFutHH310x9tevnx5sf7QQw8V6yMjIx1vezJjnB1IjrADSRB2IAnCDiRB2IEkCDuQBGEHkmCcHUVnnnlmsX7PPfcU6xdf3Plkv6tXry7WV6xYUay/9957HW/7cNbxOLvtB2zvtL1lzLLbbL9ne3P1c3mdzQKo30RO438h6dJxlv9LRMyrfn5Tb1sA6tY27BGxQdKuPvQCoIe6+YBuqe3XqtP841o9yfYS28O2W/8zMgA912nYfybpFEnzJI1IurvVEyNiKCLOjoizO9wWgBp0FPaI2BER+yPigKSfS5pfb1sA6tZR2G3PHPNwoaQtrZ4LYDC0HWe3vVbS9yUdL2mHpJ9Wj+dJCknvSvpxRLS9uJhx9sln2rRpxfqVV17ZstbuWnl73OHir6xfv75YX7BgQbE+WbUaZz9iAisuGmfx/V13BKCv+LoskARhB5Ig7EAShB1IgrADSXCJKxrzxRdfFOtHHFEeLNq3b1+xfskll7SsPf/888V1D2f8K2kgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSKLtVW/I7ayzzirWr7vuumL9nHPOaVlrN47eztatW4v1DRs2dPX6kw1HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2SW7u3LnF+tKlS4v1a665plg/8cQTD7mnidq/f3+xPjJS/u/lBw4cqLOdwx5HdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2w0C7sexFi8abaHdUu3H02bNnd9JSLYaHh4v1FStWFOvr1q2rs51Jr+2R3fZJtp+zvc32G7ZvqZZPt/2M7beq2+N63y6ATk3kNH6fpL+PiD+X9FeSbrZ9uqRbJT0bEadKerZ6DGBAtQ17RIxExKbq/h5J2yTNknSVpDXV09ZIurpHPQKowSG9Z7c9W9L3JG2UNCMiRqTRPwi2T2ixzhJJS7rsE0CXJhx229+W9Iikn0TEx/a4c8d9Q0QMSRqqXoOJHYGGTGjozfa3NBr0X0bEo9XiHbZnVvWZknb2pkUAdWh7ZPfoIfx+Sdsi4p4xpXWSbpC0srp9vCcdTgIzZswo1k8//fRi/d577y3WTzvttEPuqS4bN24s1u+8886WtccfL//KcIlqvSZyGn++pL+V9LrtzdWy5RoN+a9tL5b0e0k/7EmHAGrRNuwR8Z+SWr1Bv7jedgD0Cl+XBZIg7EAShB1IgrADSRB2IAkucZ2g6dOnt6ytXr26uO68efOK9Tlz5nTSUi1efPHFYv3uu+8u1p966qli/bPPPjvkntAbHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIk04+znnntusb5s2bJiff78+S1rs2bN6qinunz66acta6tWrSque8cddxTre/fu7agnDB6O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRJpx9oULF3ZV78bWrVuL9SeffLJY37dvX7FeuuZ89+7dxXWRB0d2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUjCEVF+gn2SpIcknSjpgKShiPhX27dJuknS/1ZPXR4Rv2nzWuWNAehaRIw76/JEwj5T0syI2GT7aEkvS7pa0t9I+iQi7ppoE4Qd6L1WYZ/I/Owjkkaq+3tsb5PU7L9mAXDIDuk9u+3Zkr4naWO1aKnt12w/YPu4FusssT1se7i7VgF0o+1p/FdPtL8t6QVJKyLiUdszJH0gKST9k0ZP9W9s8xqcxgM91vF7dkmy/S1JT0p6KiLuGac+W9KTEXFmm9ch7ECPtQp729N425Z0v6RtY4NefXB30EJJW7ptEkDvTOTT+Ask/Yek1zU69CZJyyUtkjRPo6fx70r6cfVhXum1OLIDPdbVaXxdCDvQex2fxgOYHAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ9HvK5g8k/c+Yx8dXywbRoPY2qH1J9NapOnv7s1aFvl7P/o2N28MRcXZjDRQMam+D2pdEb53qV2+cxgNJEHYgiabDPtTw9ksGtbdB7Uuit071pbdG37MD6J+mj+wA+oSwA0k0Enbbl9p+0/bbtm9toodWbL9r+3Xbm5uen66aQ2+n7S1jlk23/Yztt6rbcefYa6i322y/V+27zbYvb6i3k2w/Z3ub7Tds31Itb3TfFfrqy37r+3t221Mk/U7SAknbJb0kaVFEbO1rIy3YflfS2RHR+BcwbP+1pE8kPXRwai3b/yxpV0SsrP5QHhcR/zAgvd2mQ5zGu0e9tZpm/O/U4L6rc/rzTjRxZJ8v6e2IeCcivpT0K0lXNdDHwIuIDZJ2fW3xVZLWVPfXaPSXpe9a9DYQImIkIjZV9/dIOjjNeKP7rtBXXzQR9lmS/jDm8XYN1nzvIelp2y/bXtJ0M+OYcXCarer2hIb7+bq203j309emGR+YfdfJ9OfdaiLs401NM0jjf+dHxF9KukzSzdXpKibmZ5JO0egcgCOS7m6ymWqa8Uck/SQiPm6yl7HG6asv+62JsG+XdNKYx9+R9H4DfYwrIt6vbndKekyjbzsGyY6DM+hWtzsb7ucrEbEjIvZHxAFJP1eD+66aZvwRSb+MiEerxY3vu/H66td+ayLsL0k61fZ3bR8p6UeS1jXQxzfYnlp9cCLbUyX9QIM3FfU6STdU92+Q9HiDvfyRQZnGu9U042p43zU+/XlE9P1H0uUa/UT+vyX9YxM9tOhrjqRXq583mu5N0lqNntb9n0bPiBZL+lNJz0p6q7qdPkC9/ZtGp/Z+TaPBmtlQbxdo9K3ha5I2Vz+XN73vCn31Zb/xdVkgCb5BByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ/D+f1mbt6t55/AAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label:', label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## torchvision.transforms\n",
    "make the MNIST PIL.image object to value data(tensors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) 5\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset = MNIST(root='data/',\n",
    "                train=True,\n",
    "                transform=transforms.ToTensor())\n",
    "\n",
    "img_tensor, label = dataset[0]\n",
    "print(img_tensor.shape, label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(img_tensor[:, 10:15, 10:15])\n",
    "print(torch.max(img_tensor), torch.min(img_tensor))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7f1701d006a0>"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_tensor[0, 10:15, 10:15], cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split dataset to 3 parts\n",
    "1. Training set\n",
    "2. Validation set\n",
    "3. Testing Set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split validation and training by hand\n",
    "And we shuffle the data first(by index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def spilit_indices(n, val_percent):\n",
    "\n",
    "    validation_n = int(val_percent * n)\n",
    "\n",
    "    # just shuffle the list of [0,1,...,n-1]\n",
    "    idxs = np.random.permutation(n)\n",
    "\n",
    "    # [n : ] skip first n\n",
    "    # [ : n] from 0 to n\n",
    "    return idxs[validation_n:], idxs[:validation_n]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 12000\n",
      "Smaple validation indices:  [20653  3064 35647 14774 45878 45664   986 38396 54585 29738 21554 42249\n",
      " 12253 15214 12531 54810  4952 34688 41226 54257]\n"
     ]
    }
   ],
   "source": [
    "train_indices, validation_indices = spilit_indices(len(dataset), val_percent=0.2)\n",
    "\n",
    "print(len(train_indices), len(validation_indices))\n",
    "print('Smaple validation indices: ', validation_indices[:20])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`SubsetRandomSampler` using indices to sample data\n",
    "and randomly, so the shuffle indices might be redundant."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Combine sampler(using indices) and data loader\n",
    "# Training\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "train_loader = DataLoader(dataset,\n",
    "                          batch_size,\n",
    "                          sampler=train_sampler)\n",
    "\n",
    "# Validation\n",
    "val_sampler = SubsetRandomSampler(validation_indices)\n",
    "val_loader = DataLoader(dataset,\n",
    "                        batch_size,\n",
    "                        sampler=val_sampler)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## split data using utils\n",
    "`torch.utils.data.random_split`\n",
    "\n",
    "can split dataset into validation set and training set as well"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 10000)"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spliting Training and Validation datasets\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic regression model\n",
    "* almost like linear regression\n",
    "* we **flatten** the 1x28x28 image tensor to 784\n",
    "* output is vector of size 10, signifying the probability of target label(from 0 to 9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_size = 1*28*28\n",
    "num_classes = 10\n",
    "\n",
    "# Logistic regression model\n",
    "model = nn.Linear(input_size, num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight:\n",
      "torch.Size([10, 784])\n",
      "Parameter containing:\n",
      "tensor([[-0.0347, -0.0301, -0.0073,  ...,  0.0262,  0.0319,  0.0213],\n",
      "        [ 0.0260,  0.0300,  0.0127,  ..., -0.0169,  0.0012,  0.0079],\n",
      "        [-0.0166, -0.0157,  0.0162,  ...,  0.0108, -0.0077, -0.0246],\n",
      "        ...,\n",
      "        [-0.0229, -0.0093,  0.0236,  ..., -0.0345, -0.0288,  0.0030],\n",
      "        [ 0.0309,  0.0052, -0.0119,  ..., -0.0258,  0.0255, -0.0114],\n",
      "        [ 0.0062, -0.0209, -0.0189,  ...,  0.0334, -0.0210, -0.0031]],\n",
      "       requires_grad=True)\n",
      "\n",
      "bias:\n",
      "torch.Size([10])\n",
      "Parameter containing:\n",
      "tensor([ 0.0130, -0.0276, -0.0009, -0.0019,  0.0214, -0.0306, -0.0144,  0.0006,\n",
      "         0.0266,  0.0258], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# checkout the weight and bias\n",
    "print(\"weight:\")\n",
    "print(model.weight.shape)\n",
    "print(model.weight)\n",
    "\n",
    "print()\n",
    "print('bias:')\n",
    "print(model.bias.shape)\n",
    "print(model.bias)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 5, 7, 4, 8, 5, 6, 0, 8, 8, 0, 4, 4, 3, 3, 5, 4, 3, 3, 1, 2, 5, 6, 3,\n",
      "        5, 4, 3, 3, 9, 4, 9, 7, 6, 9, 6, 1, 3, 3, 1, 1, 4, 7, 2, 8, 5, 9, 6, 6,\n",
      "        9, 1, 3, 4, 3, 6, 9, 7, 8, 9, 3, 9, 2, 6, 0, 9, 8, 0, 8, 0, 7, 7, 1, 6,\n",
      "        6, 4, 4, 9, 7, 1, 2, 8, 0, 6, 9, 1, 2, 9, 1, 8, 3, 3, 6, 0, 0, 4, 6, 5,\n",
      "        1, 3, 9, 2, 2, 2, 9, 1, 1, 4, 9, 4, 1, 3, 1, 3, 4, 6, 7, 2, 2, 2, 9, 1,\n",
      "        0, 6, 0, 2, 1, 2, 9, 6])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-120-b068d2074c9a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0;31m# because the size of input has not been flatten\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m     \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m     \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch-env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1049\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1050\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1051\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1052\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1053\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch-env/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 96\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     97\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     98\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mextra_repr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch-env/lib/python3.6/site-packages/torch/nn/functional.py\u001B[0m in \u001B[0;36mlinear\u001B[0;34m(input, weight, bias)\u001B[0m\n\u001B[1;32m   1845\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mhas_torch_function_variadic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1846\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mhandle_torch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1847\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlinear\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1848\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1849\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (3584x28 and 784x10)"
     ]
    }
   ],
   "source": [
    "# print every label\n",
    "for images, labels in train_loader:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "\n",
    "    # because the size of input has not been flatten\n",
    "    outputs = model(images)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "# we make a model wrap linear model\n",
    "# and we flatten first and put into linear model after\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        # -1 means x, a variable\n",
    "        xb = xb.reshape(-1, 784)\n",
    "        out = self.linear(xb)\n",
    "\n",
    "        return out\n",
    "\n",
    "model = MnistModel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.shape :  torch.Size([128, 10])\n",
      "Smaple outputs : \n",
      " tensor([ 0.3996, -0.3318, -0.1151,  0.2781, -0.0193,  0.2117, -0.1346,  0.3927,\n",
      "        -0.1248, -0.0184], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    outputs = model(images)\n",
    "    break\n",
    "\n",
    "print('outputs.shape : ', outputs.shape)\n",
    "print('Smaple outputs : \\n', outputs[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output we expect to get is the probabilities.\n",
    "However, the output here significantly is not probabilities.\n",
    "\n",
    "So we can put it into **Softmax**\n",
    "Convert logic score to probabilities."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prob : \n",
      " tensor([0.1374, 0.0661, 0.0821, 0.1216, 0.0903, 0.1138, 0.0805, 0.1364, 0.0813,\n",
      "        0.0904])\n",
      "Sum:  1.0\n"
     ]
    }
   ],
   "source": [
    "import  torch.nn.functional as F\n",
    "\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "# checkout sample prob.\n",
    "print(\"Sample prob : \\n\", probs[0].data)\n",
    "\n",
    "# add up the prob of an output row\n",
    "# verify if equal to 1\n",
    "print('Sum: ', torch.sum(probs[0]).item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 7, 7, 3, 3, 5, 7, 1, 5, 0, 0, 3, 7, 5, 3, 7, 3, 7, 3, 7, 3, 3, 3, 3,\n",
      "        3, 7, 7, 3, 3, 7, 3, 3, 7, 7, 7, 7, 7, 3, 5, 3, 7, 4, 0, 3, 3, 3, 3, 7,\n",
      "        7, 3, 8, 3, 7, 3, 3, 3, 7, 3, 9, 3, 3, 5, 0, 0, 7, 0, 3, 7, 3, 7, 7, 3,\n",
      "        7, 7, 3, 3, 0, 3, 8, 3, 7, 3, 3, 3, 3, 3, 3, 7, 7, 1, 7, 0, 7, 3, 0, 3,\n",
      "        3, 8, 7, 3, 7, 9, 3, 3, 3, 3, 0, 0, 7, 7, 3, 0, 7, 7, 3, 3, 4, 3, 2, 3,\n",
      "        7, 7, 0, 3, 7, 3, 3, 3])\n",
      "tensor([0.1374, 0.1346, 0.1408, 0.1430, 0.1759, 0.1287, 0.1491, 0.1297, 0.1295,\n",
      "        0.1488, 0.1245, 0.1361, 0.1195, 0.1434, 0.1724, 0.1367, 0.1552, 0.1460,\n",
      "        0.1404, 0.1354, 0.1530, 0.1535, 0.1634, 0.1757, 0.1238, 0.1509, 0.1594,\n",
      "        0.1715, 0.1443, 0.1360, 0.1505, 0.1226, 0.1658, 0.1400, 0.1455, 0.1237,\n",
      "        0.1238, 0.1600, 0.1205, 0.1470, 0.1545, 0.1332, 0.1310, 0.1549, 0.1239,\n",
      "        0.1488, 0.1498, 0.1604, 0.1280, 0.1550, 0.1299, 0.1525, 0.1317, 0.1765,\n",
      "        0.1415, 0.2003, 0.1270, 0.1539, 0.1621, 0.1536, 0.1623, 0.1390, 0.1531,\n",
      "        0.1187, 0.1410, 0.1402, 0.1271, 0.1420, 0.1732, 0.1414, 0.1442, 0.1268,\n",
      "        0.1437, 0.1437, 0.1295, 0.1255, 0.1295, 0.1698, 0.1238, 0.1330, 0.1452,\n",
      "        0.1346, 0.1296, 0.1330, 0.1470, 0.1399, 0.1439, 0.1591, 0.1322, 0.1310,\n",
      "        0.1316, 0.1209, 0.1404, 0.1319, 0.1301, 0.1295, 0.1369, 0.1272, 0.1390,\n",
      "        0.1445, 0.1505, 0.1215, 0.1314, 0.1693, 0.1587, 0.1447, 0.1185, 0.1250,\n",
      "        0.1411, 0.1473, 0.1669, 0.1285, 0.1384, 0.1326, 0.1533, 0.1741, 0.1286,\n",
      "        0.1479, 0.1304, 0.1390, 0.1348, 0.1456, 0.1412, 0.1264, 0.1275, 0.1435,\n",
      "        0.1523, 0.1294], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "max_probs, preds = torch.max(probs, dim=1)\n",
    "print(preds)\n",
    "print(max_probs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Metric and Loss\n",
    "we use accuracy here for evaluation metric.\n",
    "\n",
    "acc is a great way for humans to evaluate model\n",
    "however it can't use as a loss function by non-differentiable.\n",
    "\n",
    "so we use **Cross entropy** in real loss function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0703125"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(label_pred, label_targ):\n",
    "    return torch.sum(label_pred == label_targ).item() / len(label_pred)\n",
    "\n",
    "accuracy(preds, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3844, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "\n",
    "# F.cross_entropy average all the cross entropy of output\n",
    "loss = loss_fn(outputs, labels)\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizer\n",
    "`optim.SGD` here"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate Loss and Evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None, metric=None):\n",
    "\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb)\n",
    "    # loss_func return a 1d tensor\n",
    "    # we can get value by loss.item()\n",
    "\n",
    "    if opt is not None:\n",
    "        # compute gradient\n",
    "        loss.backward()\n",
    "        # optimizer upgrade params\n",
    "        opt.step()\n",
    "        # reset gradient\n",
    "        opt.zero_grad()\n",
    "\n",
    "    metric_result = None\n",
    "    if metric is not None:\n",
    "\n",
    "        metric_result = metric(preds, yb)\n",
    "\n",
    "    return loss.item(), len(xb), metric_result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "def evaluate(model, loss_func, valid_dataloader, metric=None):\n",
    "    # temporarily set requires_grad to false\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # results is a list of results\n",
    "        # results = [(loss, len of batch ,metric) ... ]\n",
    "        results = [loss_batch(model, loss_func, xb, yb, metric=metric)\n",
    "                    for xb, yb in valid_dataloader]\n",
    "\n",
    "        # take one list of 3 element tuple (loss, len, metric)\n",
    "        # to 3 list\n",
    "        losses, nums, metrics = zip(*results)\n",
    "\n",
    "        total= np.sum(nums)\n",
    "\n",
    "        avg_loss = np.sum(np.multiply(losses, nums)) / total\n",
    "        avg_metric = None\n",
    "        if metric is not None:\n",
    "\n",
    "            avg_metric = np.sum(np.multiply(metrics, nums)) / total\n",
    "\n",
    "        return avg_loss, total, avg_metric"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.sum(preds == labels).item() / len(preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x7f16bbe33308>\n",
      "Loss: 1.1939, Accuracy: 81.04%\n"
     ]
    }
   ],
   "source": [
    "val_loss, total, val_acc = evaluate(model, loss_fn,val_loader, metric=accuracy)\n",
    "print('Loss: {:.4f}, Accuracy: {:.2f}%'.format(val_loss, val_acc*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_fn, opt, train_dataloader, valid_dataloader, metric=None):\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        for xb, yb in train_dataloader:\n",
    "            loss, _, _ = loss_batch(model, loss_fn, xb, yb, opt)\n",
    "\n",
    "        # Evaluation\n",
    "        result = evaluate(model, loss_fn, valid_dataloader, metric)\n",
    "        val_loss, total, val_metric = result\n",
    "\n",
    "        # print progress\n",
    "        if metric is None:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                    .format(epoch+1, epochs, val_loss))\n",
    "\n",
    "        else:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}, {}: {:.4f}'\n",
    "                    .format(epoch+1, epochs, val_loss, metric.__name__, val_metric))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.9680, accuracy: 0.5521\n",
      "Epoch [2/5], Loss: 1.6934, accuracy: 0.7249\n",
      "Epoch [3/5], Loss: 1.4875, accuracy: 0.7677\n",
      "Epoch [4/5], Loss: 1.3318, accuracy: 0.7906\n",
      "Epoch [5/5], Loss: 1.2121, accuracy: 0.8058\n"
     ]
    }
   ],
   "source": [
    "model = MnistModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "fit(epochs=5,\n",
    "    model=model,\n",
    "    loss_fn=F.cross_entropy,\n",
    "    opt=optimizer,\n",
    "    train_dataloader=train_loader,\n",
    "    valid_dataloader=val_loader,\n",
    "    metric=accuracy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing Single IMAGE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "test_dataset = MNIST(root='data/',\n",
    "                     train=False,\n",
    "                     transform=transforms.ToTensor())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    xb = img.unsqueeze(0)\n",
    "    yb = model(xb)\n",
    "    _, preds = torch.max(yb, dim=1)\n",
    "    return preds[0].item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 7 Predicted: 7\n",
      "Label: 0 Predicted: 0\n",
      "Label: 9 Predicted: 7\n",
      "Label: 6 Predicted: 6\n",
      "Label: 2 Predicted: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_index = [0, 10, 20, 50, 77]\n",
    "\n",
    "for i in test_index:\n",
    "    img, label = test_dataset[i]\n",
    "    plt.figure()\n",
    "    plt.imshow(img[0], cmap='gray')\n",
    "    print('Label:', label, 'Predicted:', predict_image(img, model) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get final test result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2483)\n",
      "tensor(1.2690)\n",
      "tensor(1.2672)\n",
      "tensor(1.2134)\n",
      "tensor(1.3515)\n",
      "tensor(1.2967)\n",
      "tensor(1.3179)\n",
      "tensor(1.2690)\n",
      "tensor(1.3504)\n",
      "tensor(1.2549)\n",
      "tensor(1.2626)\n",
      "tensor(1.2342)\n",
      "tensor(1.2422)\n",
      "tensor(1.2766)\n",
      "tensor(1.3124)\n",
      "tensor(1.2617)\n",
      "tensor(1.2980)\n",
      "tensor(1.2981)\n",
      "tensor(1.2568)\n",
      "tensor(1.1955)\n",
      "tensor(1.0410)\n",
      "tensor(1.0316)\n",
      "tensor(1.0614)\n",
      "tensor(1.1816)\n",
      "tensor(0.9091)\n",
      "tensor(1.1827)\n",
      "tensor(1.1883)\n",
      "tensor(1.0636)\n",
      "tensor(1.1163)\n",
      "tensor(1.2408)\n",
      "tensor(1.0151)\n",
      "tensor(0.9845)\n",
      "tensor(1.1326)\n",
      "tensor(0.9758)\n",
      "tensor(0.8823)\n",
      "tensor(1.1036)\n",
      "tensor(1.0624)\n",
      "tensor(1.1760)\n",
      "tensor(1.3915)\n",
      "tensor(1.0143)\n",
      "(1.1847789430618285, 10000, 0.8131)\n",
      "Acc: 81.31%\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size = 256)\n",
    "result = evaluate(model,\n",
    "                  loss_func=loss_fn,\n",
    "                  valid_dataloader=test_loader,\n",
    "                  metric=accuracy)\n",
    "\n",
    "print(result)\n",
    "print('Acc: {:.2f}%'.format(result[2]*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[-0.0264, -0.0056, -0.0250,  ...,  0.0090,  0.0248,  0.0032],\n",
      "        [ 0.0052, -0.0100,  0.0231,  ..., -0.0211,  0.0235, -0.0266],\n",
      "        [-0.0311, -0.0286, -0.0323,  ...,  0.0209,  0.0158,  0.0267],\n",
      "        ...,\n",
      "        [ 0.0016,  0.0218,  0.0058,  ..., -0.0267, -0.0077, -0.0240],\n",
      "        [ 0.0088, -0.0313,  0.0114,  ..., -0.0175, -0.0028,  0.0047],\n",
      "        [-0.0187,  0.0203, -0.0165,  ..., -0.0348,  0.0162, -0.0183]])), ('linear.bias', tensor([ 0.0065,  0.0107, -0.0078, -0.0085,  0.0108, -0.0102,  0.0034,  0.0156,\n",
      "        -0.0098,  0.0006]))])\n"
     ]
    }
   ],
   "source": [
    "model_file_name = 'mnist-logistic.pth'\n",
    "torch.save(model.state_dict(), model_file_name)\n",
    "print(model.state_dict())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[-0.0132, -0.0104, -0.0161,  ...,  0.0347,  0.0269,  0.0281],\n",
      "        [ 0.0023, -0.0124, -0.0353,  ...,  0.0222,  0.0188,  0.0083],\n",
      "        [ 0.0322, -0.0269, -0.0051,  ..., -0.0013,  0.0223,  0.0067],\n",
      "        ...,\n",
      "        [-0.0176, -0.0015, -0.0101,  ...,  0.0033,  0.0297,  0.0320],\n",
      "        [-0.0347,  0.0138, -0.0045,  ..., -0.0010,  0.0239, -0.0054],\n",
      "        [ 0.0196,  0.0295, -0.0173,  ...,  0.0246, -0.0071,  0.0124]])), ('linear.bias', tensor([ 0.0138, -0.0223,  0.0276,  0.0245,  0.0177, -0.0143,  0.0182,  0.0046,\n",
      "         0.0051,  0.0173]))])\n",
      "tensor(2.2995)\n",
      "tensor(2.2681)\n",
      "tensor(2.2853)\n",
      "tensor(2.2688)\n",
      "tensor(2.2968)\n",
      "tensor(2.2626)\n",
      "tensor(2.2877)\n",
      "tensor(2.2849)\n",
      "tensor(2.2796)\n",
      "tensor(2.2647)\n",
      "tensor(2.2727)\n",
      "tensor(2.2880)\n",
      "tensor(2.2776)\n",
      "tensor(2.2821)\n",
      "tensor(2.2776)\n",
      "tensor(2.2823)\n",
      "tensor(2.2782)\n",
      "tensor(2.2861)\n",
      "tensor(2.2862)\n",
      "tensor(2.2702)\n",
      "tensor(2.2620)\n",
      "tensor(2.2650)\n",
      "tensor(2.2539)\n",
      "tensor(2.2485)\n",
      "tensor(2.3063)\n",
      "tensor(2.2977)\n",
      "tensor(2.3074)\n",
      "tensor(2.3069)\n",
      "tensor(2.2854)\n",
      "tensor(2.2952)\n",
      "tensor(2.2809)\n",
      "tensor(2.2865)\n",
      "tensor(2.3313)\n",
      "tensor(2.2757)\n",
      "tensor(2.2886)\n",
      "tensor(2.2833)\n",
      "tensor(2.2786)\n",
      "tensor(2.3096)\n",
      "tensor(2.2912)\n",
      "tensor(2.2526)\n",
      "Origin Acc: 13.02%\n",
      "\n",
      "OrderedDict([('linear.weight', tensor([[-0.0264, -0.0056, -0.0250,  ...,  0.0090,  0.0248,  0.0032],\n",
      "        [ 0.0052, -0.0100,  0.0231,  ..., -0.0211,  0.0235, -0.0266],\n",
      "        [-0.0311, -0.0286, -0.0323,  ...,  0.0209,  0.0158,  0.0267],\n",
      "        ...,\n",
      "        [ 0.0016,  0.0218,  0.0058,  ..., -0.0267, -0.0077, -0.0240],\n",
      "        [ 0.0088, -0.0313,  0.0114,  ..., -0.0175, -0.0028,  0.0047],\n",
      "        [-0.0187,  0.0203, -0.0165,  ..., -0.0348,  0.0162, -0.0183]])), ('linear.bias', tensor([ 0.0065,  0.0107, -0.0078, -0.0085,  0.0108, -0.0102,  0.0034,  0.0156,\n",
      "        -0.0098,  0.0006]))])\n",
      "tensor(1.2483)\n",
      "tensor(1.2690)\n",
      "tensor(1.2672)\n",
      "tensor(1.2134)\n",
      "tensor(1.3515)\n",
      "tensor(1.2967)\n",
      "tensor(1.3179)\n",
      "tensor(1.2690)\n",
      "tensor(1.3504)\n",
      "tensor(1.2549)\n",
      "tensor(1.2626)\n",
      "tensor(1.2342)\n",
      "tensor(1.2422)\n",
      "tensor(1.2766)\n",
      "tensor(1.3124)\n",
      "tensor(1.2617)\n",
      "tensor(1.2980)\n",
      "tensor(1.2981)\n",
      "tensor(1.2568)\n",
      "tensor(1.1955)\n",
      "tensor(1.0410)\n",
      "tensor(1.0316)\n",
      "tensor(1.0614)\n",
      "tensor(1.1816)\n",
      "tensor(0.9091)\n",
      "tensor(1.1827)\n",
      "tensor(1.1883)\n",
      "tensor(1.0636)\n",
      "tensor(1.1163)\n",
      "tensor(1.2408)\n",
      "tensor(1.0151)\n",
      "tensor(0.9845)\n",
      "tensor(1.1326)\n",
      "tensor(0.9758)\n",
      "tensor(0.8823)\n",
      "tensor(1.1036)\n",
      "tensor(1.0624)\n",
      "tensor(1.1760)\n",
      "tensor(1.3915)\n",
      "tensor(1.0143)\n",
      "Loaded Acc: 81.31%\n"
     ]
    }
   ],
   "source": [
    "model_origin = MnistModel()\n",
    "print(model_origin.state_dict())\n",
    "\n",
    "origin_met = evaluate(model_origin,\n",
    "                  loss_func=loss_fn,\n",
    "                  valid_dataloader=test_loader,\n",
    "                  metric=accuracy)\n",
    "print('Origin Acc: {:.2f}%'.format(origin_met[2]*100))\n",
    "\n",
    "print()\n",
    "model_origin.load_state_dict(torch.load(model_file_name))\n",
    "print(model_origin.state_dict())\n",
    "\n",
    "origin_met = evaluate(model_origin,\n",
    "                  loss_func=loss_fn,\n",
    "                  valid_dataloader=test_loader,\n",
    "                  metric=accuracy)\n",
    "print('Loaded Acc: {:.2f}%'.format(origin_met[2]*100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda availability : True\n",
      "How many cuda device : 1\n",
      "Name of current device : NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# TEST GPU\n",
    "import torch\n",
    "\n",
    "print('Cuda availability :',torch.cuda.is_available())\n",
    "print('How many cuda device :', torch.cuda.device_count())\n",
    "cuda_device = torch.cuda.current_device()\n",
    "print('Name of current device :',torch.cuda.get_device_name(cuda_device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
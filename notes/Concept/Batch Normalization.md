 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
 
 *internal covariate shift*
 
 ![](https://i.imgur.com/IyoeroQ.png)
(2014 Inception network)

![](https://i.imgur.com/n11i0He.png)
![](https://i.imgur.com/3iFARWo.png)


![](https://i.imgur.com/SUFwFaX.png)


 Tips:
* Large learning rate
* don't use with dropout

refer:
[https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/)
[https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739](https://towardsdatascience.com/batch-norm-explained-visually-how-it-works-and-why-neural-networks-need-it-b18919692739)